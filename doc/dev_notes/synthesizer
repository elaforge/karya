  vim: set ai sw=2 foldmethod=indent tw=78 :

Learn about physical modeling techniques.
  - Julius O Smith book on waveguides, Bilbao book on finite difference.
  - Transcribe STK models.
  - Other techniques: subtractive, waveshaping, FM, granular, etc.  Check out
    Henning's synthesizer-llvm for implementations.

Figure out how to write synthesizers.
  Looks like the options are haskell + llvm as as sort of high level
  assembler, FAUST, or pure haskell (e.g. jwlato's dsp-extras).  I need to
  learn more about them to judge.
  - Haskell + LLVM: Good performance, but more awkward to write in than pure
    haskell.
  - FAUST: Good performance, but likely even more awkward than llvm since it
    has to go to C++ and then needs a wrapper to interact with haskell.
    Some people dislike the syntax.
  - pure haskell: Has the smallest impedence mismatch, but experimental and
    unpredictable performance.  Even at its best probably won't be as good as
    LLVM or FAUST.
  - http://halide-lang.org/ compile down to fast object code, hasn't been used
    for audio but should be able.  Also C++ like FAUST.

Write OSC synthesizer host.
  osc:
    bundle:
      /inst/0/on <duration>
      /inst/0/c 'name' samples

    Configuration:
    /alloc 'inst' [ param param param ... ]

  I think OSC doesn't have an efficient way to communicate control data, since
  I'd wind up with either thousands of messages, or one message with thousands
  of type parameters (timestamp, value, timestamp, value, etc.), and existing
  OSC libraries are probably not designed to handle that efficiently.

  I can send controls as the control name followed by a blob, which is times
  and values, in a format that I can pass directly to the synthesizer.

  Control values should be able to change every sample, but I don't want to
  send that much data.  The key thing is that the sequencer itself doesn't
  store that kind of high level data.  Really the only distinction is linear
  interpolation between samples, or change as quickly as possible (presumably
  still with some interpolation, to avoid clicks).  But it might be simple
  enough to just always change as quickly as possible, and send more samples
  if it's audibly jagged.

  AUs have a complicated way of configuring per-sample control changes, but
  who knows if any of them actually support that API.

How to do a non-realtime synthesizer:
  - Messages are sent in advance.  I can stream them with 30s or so of buffer,
    to if the render is aborted I don't have to send the rest of the notes.
  cache
    - To avoid rerendering unnecessarily, I can cache previous renders.
      But this means the sequencer also needs to send cache-invalidate msgs.
      The trick then is turn score damage into (instrument, time_range).
    - Or I could do it memoize style, by remembering the complete set of msgs
      for each note.  If I receive the same set of msgs I can just directly
      return the sound.  This requires some kind of GC to time-out old samples
      since there's nothing explicitly expiring them.  But disk space is cheap.
    - The granularity of the cache depends on independent notes.  So if each
      note id is independent, then I can cache each note.  If the notes on the
      same instrument interact, then I can only cache when all notes are off
      and all ringing has died down.  But this may never happen, so maybe I
      should set a cutoff after a certain amount of time.  When I rerender the
      score from scratch I can do the expensive one-giant-note approach.

    - The sequencer has to start sending OSC over as soon as the score is
      derived, because it doesn't know where I will start playback.  The
      explicit cache-expiry approach would require me to send across only the
      bits that changed, and hope that's accurate.  The memoize approach would
      require the whole score, and the synthesizer would decide which bits
      changed.  But sending the whole score will get progressively slower as I
      edit the end of it.  On the other hand, the synthesizer might want to go
      back a bit to the start of a sequence of non-overlapping notes (e.g. if
      the model is a cymbal crash, caching individual notes degenerates to
      playing samples).  How much interaction each note has depends on the
      instrument, so this is something the synthesizer knows best.  If it can
      communicate with the sequencer it can ask for the notes it needs.

    - Actually, the important thing is the cache grouping.  The sequencer can
      use that same grouping to avoid sending cached notes.  So, sequencer
      sends the entire score.  The synthesizer breaks that up into chunks, and
      renders a sample for each one (it can also parallelize).  It tells the
      sequencer about the chunks (either explicitly, or by creating
      placeholder files in the cache).  Then the sequencer has
      [(instrument, time_range)], and can use that to send only the events
      that lie within it.  The synthesizer then deletes all caches that
      encompass each note it receives.

    - It's interesting to think about how memoizing degenerates into sampling.
      If I memoize individual notes and say each note is independent of
      starting time then I gradually accumulate a sample set.

  - The sound still has to get to a DAW so I can apply effects and mix with the
    other instruments.
  - Ways to get audio into a DAW include JACK or VST.  If I used JACK, I'd
    need a server to schedule and send over the audio, and JACK support is
    still awkward in everything except ardour, so VST it is.
  - I think I need a sample player VST that understands the structure of the
    synthesizer cache.  The sequencer sends an instrument and a start play
    time, and it starts playing the instrument output from that point in time.
    I'd need to somehow encode instrument and start into MIDI, but there are
    lots of hacks^Wpossibilities, e.g. maintain a MIDI channel to instrument
    mapping, and then encode 14 bits of time into the NoteOn pitch and
    velocity.

  - If I have the synthesizer cache visible to the sequencer, I can integrate
    it more directly: I could display the samples inline with the events,
    or have sample-level transformers.

Benefits of not using MIDI:
  - Per-note addressing, so no more awkward and inefficient banks of VSTs.
  - No tricky managing channel state and endless channel sharing bugs.
  - High resolution controls.
  - Push decisions up to the sequencer, e.g. pitch variation, envelope
    variation.
  - Non-realtime means I can have expensive instruments, and no voice limit.
  - Multiple pitch signals, for e.g. fundemantal + filter.

global
  instrument resonators:
    - filters and eq
    - reverb / resonator
    - convolution / impulse response

sampler
  - Pick samples based on pitch, velocity, and other axes, and vary samples
    randomly within those parameters.  Sample zones overlap, using probability
    rather than a hard cutoff.
  - Per-note addressing lets me replay ringing notes.
  - Smarter exclusive samples, e.g. same hand cuts off other samples, unless
    it's the same stroke.
  - Freedom from keyswitches.  Address variations directly and symbolically.

  - For sample rate conversion I'd still want something efficient.  Surely
    there are libraries I can bind?  E.g. libsamplerate.
  - Reverb and filters available via freeverb3.

physical modeling
  string
    - repluck a ringing string
    - buzz from touching
    - curved bridge ala tambura
    - muted by damping at the bridge
    - bowing
    - harmonics
    - sympathetic strings

  metal, pitched and unpitched
    - muted
    - resonator
    - sympathetic strings
    - cymbals / cengceng

  drums
    - various tensions and skin thickness
    - hand damping
    - center weighting
    - buzzing like snares or kutchi
    - pitch changing via pressure ala kendang sunda or gumiki
    - edge damping



protocol
  Note protocol:
    data Note = Note
      { instrument :: InstrumentName, start :: RealTime, duration :: RealTime
      , controls :: Map Text Signal -- e.g. "pitch", "dynamic", "lpf", ...
      , articulation :: Text
      , track :: Text -- ^ used to disable / enable by track
        -- maybe this means I dont need instrument?
      }
    . Times are absolute from score start.
    . It's the job of the sampler script to convert Random -> Note -> [Sample]
      data Sample = Sample
        { start :: RealTime, duration :: RealTime
        , sample :: Text, ratio :: Signal -- ^ sample rate conversion ratio
        , pan :: Double -- ^ from 0 to 1
        , offset :: RealTime -- ^ sample start offset
        , envelope :: Signal
        }
    . So it has to choose the sample based on pitch, dyn, articulation,
      randomization, etc.  It can emit multiple samples for attack, velocity
      crossfade, or whatever else.  It can also modify the envelope based on
      stop groups, mute articulation, or whatever else.
    . Signals are [(RealTime, Double)], and are interpolated to per-sample
      when rendered.  So I can send sharp transitions if needed, but tracklang
      has a constant sample rate so maybe I need special support?  Or not,
      since I can set samples at specific points if needed.
    . Sampler then interprets Samples, applies sample rate conversion,
      envelope, and mixing, and writes them into chunks in the cache.
    . Instrument definitions can be shared, but that's just
      Map InstrumentName (Set Attribute)
  Cache protocol
    . As the cache approaches a pre-rendered audio stream, I do less work
      in realtime and the start msg gets simpler.  As it gets divided up
      by time range and instrument, I do more in realtime but incremental
      rerender becomes finer grained.
    . Normally instruments don't interact with each other, so this is a good
      horizontal division.
    . But I also have solo and mute tracks, so to avoid a rerender I should
      divide by track.
    . For timewise division, I don't think I can avoid a rough transition if I
      cut arbitrarily, so it has to be by Note.  Script can have state, but if
      I save it then it's not a problem.  Or I can push it into karya for the
      same effect.  If I put effects like reverb on afterwards then notes
      should be relatively independent.  Physical modelling will have some
      state, but I can either save it, or just live with it.
    . Try to make a new cache chunk every n seconds, but a long note may force
      it to be longer.  So chunks don't have a uniform time.
    . Or I could divide up by nubmber of notes and duration.  The ideal is
      that the cpu to rerender is constant per chunk.  Probably in most cases
      that's duration, so divide based on duration.
    . However, for cheap synths like like a sampler, too much division means
      that the VST cahce playback itself is basically being the sampler, just
      without resampling.  So it could make longer chunks.  But I don't think
      that helps, because it's the streaming from disk that's expensive.
    . If I cache every track, then I'm actually caching mixed notes.
      But I think I need a more complicated system if I'm to cache at multiple
      levels.
      TODO...
    On-disk format
      . Just drop a bunch of .wav files in a directory.  Encode the track and
        times in the filename.
      . I can first create an empty file to indicate that rendering is in
        progress.
  VST protocol
    clear cache
      . Since the cache is by (track, time), invalidation has to be the same.
        When I send a new batch of Notes, I also send time ranges that the
        Notes are to replace.  I can get those from the cache.  So:
        . Score gets damaged in a certain range.
        . Karya correlates damage with cache chunks and send Notes for them.
        . Synth deletes those chunks and starts rendering new ones.
    play from a certain time, stop
      . One difficulty is that I need to start playing from anywhere with zero
        latency.
        . This is assuming that opening a file, seeking to a place, and
          reading out samples is not realtime.
        . An easy way to fulfil that is to just put the whole cache into
          memory.
        . Otherwise I need some advance warning about where playback is going
          to start.  I could send NoteOn in advance with an agreed-upon
          latency.  The only issue would be a little bit of audible lag every
          time I hit play.
      . ChannelMessage chan (NoteOn key vel)
        key+vel = 14 bits = 16384, seems like not enough resolution
        I need 24 hours * 60 * 60s = 86400 * 1000ms = 86400000 = 27 bits
        or sample accurate at 44.1k = 3810240000 = 32 bits
      . So encode a 32bit start offset in 5 controller msgs, e.g.
        (16, 7b), (17, 7b), etc.  to cue the start position.  Send a NoteOn a
        few ms later to start.
    . Also I want to support solo and mute tracks, so the cache also has to
      divide by track.  Then send Notes have to be tagged with track, and I
      need msgs to enable and disable by track.  Sysex will do since I can
      encode text.

vst
  . Write a VST that can trigger samples.  It's basically a simple streaming
    sampler, small enough that it can all be in C++.
  - Download VST headers, find some simple examples.
  - Write basic play a sample on note on.
  - Convert MIDI stop and stop to cache playback.
  - Convert MIDI to KP subset
    How to also support realtime?  Latency not so important, but shouldn't be
    too bad.

sampler
  - convert :: [Note] -> [Sample]
  - read samples via libsndfile
    . Is flac decoding fast enough to use it for on-disk format?
  - convert via libsamplerate, schedule envelope, pitch changes, and mixing
  - convert with multiple threads
  - cache handling
    . Since script can have state (e.g. randomization), doesn't that mess up
      start from any time?  But if I save the state after each note, the
      script function is deterministic.
    . Per-note computation for the sampler is actually really cheap.  

TODO
  karya protocol / KP
    . design
  Figure out how to do offline but also realtime
    . karya -> synth cache protocol
    . karya note protocol
    . MIDI protocol to have the VST host play from the cache
    . MIDI protocol for VST host to get synth to play realtime notes
      Realtime would be compromised, since I have to convert MIDI to KP
