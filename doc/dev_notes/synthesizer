  vim: set ai sw=2 foldmethod=indent tw=78 :

Learn about physical modeling techniques.
  - Julius O Smith book on waveguides, Bilbao book on finite difference.
  - Transcribe STK models.
  - Other techniques: subtractive, waveshaping, FM, granular, etc.  Check out
    Henning's synthesizer-llvm for implementations.

Figure out how to write synthesizers.
  Looks like the options are haskell + llvm as as sort of high level
  assembler, FAUST, or pure haskell (e.g. jwlato's dsp-extras).  I need to
  learn more about them to judge.
  - Haskell + LLVM: Good performance, but more awkward to write in than pure
    haskell.
  - FAUST: Good performance, but likely even more awkward than llvm since it
    has to go to C++ and then needs a wrapper to interact with haskell.
    Some people dislike the syntax.
  - pure haskell: Has the smallest impedence mismatch, but experimental and
    unpredictable performance.  Even at its best probably won't be as good as
    LLVM or FAUST.
  - http://halide-lang.org/ compile down to fast object code, hasn't been used
    for audio but should be able.  Also C++ like FAUST.

Write OSC synthesizer host.
  osc:
    bundle:
      /inst/0/on <duration>
      /inst/0/c 'name' samples

    Configuration:
    /alloc 'inst' [ param param param ... ]

  I think OSC doesn't have an efficient way to communicate control data, since
  I'd wind up with either thousands of messages, or one message with thousands
  of type parameters (timestamp, value, timestamp, value, etc.), and existing
  OSC libraries are probably not designed to handle that efficiently.

  I can send controls as the control name followed by a blob, which is times
  and values, in a format that I can pass directly to the synthesizer.

  Control values should be able to change every sample, but I don't want to
  send that much data.  The key thing is that the sequencer itself doesn't
  store that kind of high level data.  Really the only distinction is linear
  interpolation between samples, or change as quickly as possible (presumably
  still with some interpolation, to avoid clicks).  But it might be simple
  enough to just always change as quickly as possible, and send more samples
  if it's audibly jagged.

  AUs have a complicated way of configuring per-sample control changes, but
  who knows if any of them actually support that API.

How to do a non-realtime synthesizer:
  - Messages are sent in advance.  I can stream them with 30s or so of buffer,
    to if the render is aborted I don't have to send the rest of the notes.
  cache
    - To avoid rerendering unnecessarily, I can cache previous renders.
      But this means the sequencer also needs to send cache-invalidate msgs.
      The trick then is turn score damage into (instrument, time_range).
    - Or I could do it memoize style, by remembering the complete set of msgs
      for each note.  If I receive the same set of msgs I can just directly
      return the sound.  This requires some kind of GC to time-out old samples
      since there's nothing explicitly expiring them.  But disk space is cheap.
    - The granularity of the cache depends on independent notes.  So if each
      note id is independent, then I can cache each note.  If the notes on the
      same instrument interact, then I can only cache when all notes are off
      and all ringing has died down.  But this may never happen, so maybe I
      should set a cutoff after a certain amount of time.  When I rerender the
      score from scratch I can do the expensive one-giant-note approach.

    - The sequencer has to start sending OSC over as soon as the score is
      derived, because it doesn't know where I will start playback.  The
      explicit cache-expiry approach would require me to send across only the
      bits that changed, and hope that's accurate.  The memoize approach would
      require the whole score, and the synthesizer would decide which bits
      changed.  But sending the whole score will get progressively slower as I
      edit the end of it.  On the other hand, the synthesizer might want to go
      back a bit to the start of a sequence of non-overlapping notes (e.g. if
      the model is a cymbal crash, caching individual notes degenerates to
      playing samples).  How much interaction each note has depends on the
      instrument, so this is something the synthesizer knows best.  If it can
      communicate with the sequencer it can ask for the notes it needs.

    - Actually, the important thing is the cache grouping.  The sequencer can
      use that same grouping to avoid sending cached notes.  So, sequencer
      sends the entire score.  The synthesizer breaks that up into chunks, and
      renders a sample for each one (it can also parallelize).  It tells the
      sequencer about the chunks (either explicitly, or by creating
      placeholder files in the cache).  Then the sequencer has
      [(instrument, time_range)], and can use that to send only the events
      that lie within it.  The synthesizer then deletes all caches that
      encompass each note it receives.

    - It's interesting to think about how memoizing degenerates into sampling.
      If I memoize individual notes and say each note is independent of
      starting time then I gradually accumulate a sample set.

  - The sound still has to get to a DAW so I can apply effects and mix with the
    other instruments.
  - Ways to get audio into a DAW include JACK or VST.  If I used JACK, I'd
    need a server to schedule and send over the audio, and JACK support is
    still awkward in everything except ardour, so VST it is.
  - I think I need a sample player VST that understands the structure of the
    synthesizer cache.  The sequencer sends an instrument and a start play
    time, and it starts playing the instrument output from that point in time.
    I'd need to somehow encode instrument and start into MIDI, but there are
    lots of hacks^Wpossibilities, e.g. maintain a MIDI channel to instrument
    mapping, and then encode 14 bits of time into the NoteOn pitch and
    velocity.

  - If I have the synthesizer cache visible to the sequencer, I can integrate
    it more directly: I could display the samples inline with the events,
    or have sample-level transformers.

Benefits of not using MIDI:
  - Per-note addressing, so no more awkward and inefficient banks of VSTs.
  - No tricky managing channel state and endless channel sharing bugs.
  - High resolution controls.
  - Push decisions up to the sequencer, e.g. pitch variation, envelope
    variation.
  - Non-realtime means I can have expensive instruments, and no voice limit.

global
  instrument resonators:
    - filters and eq
    - reverb / resonator
    - convolution / impulse response

sampler
  - pick samples based on pitch, velocity, and other axes, and vary samples
    randomly within those parameters.
  - per-note address lets me replay ringing notes

physical modeling
  string
    - repluck a ringing string
    - buzz from touching
    - curved bridge ala tambura
    - muted by damping at the bridge
    - bowing
    - harmonics
    - sympathetic strings

  metal, pitched and unpitched
    - muted
    - resonator
    - sympathetic strings
    - cymbals / cengceng

  drums
    - various tensions and skin thickness
    - hand damping
    - center weighting
    - buzzing like snares or kutchi
    - pitch changing via pressure ala kendang sunda or gumiki
    - edge damping
